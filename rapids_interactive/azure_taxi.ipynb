{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running a DASK cluster with RAPIDS\n",
    "\n",
    "This notebook runs a DASK cluster with NVIDIA RAPIDS. RAPIDS uses NVIDIA CUDA for high-performance GPU execution, exposing GPU parallelism and high memory bandwidth through a user-friendly Python interface. It includes a dataframe library called cuDF which will be familiar to Pandas users, as well as an ML library called cuML that provides GPU versions of all machine learning algorithms available in Scikit-learn. \n",
    "\n",
    "This notebook shows how through DASK, RAPIDS can take advantage of multi-node, multi-GPU configurations on AzureML.\n",
    "\n",
    "Note: This notebook is deploying the AzureML cluster to a VNet. In this case the following names are used to identify the VNet and subnet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "vnet_resourcegroup_name='demo'\n",
    "vnet_name='myvnet'\n",
    "subnet_name='default'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "\n",
    "from azureml.core import Workspace, Experiment, Environment\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "from azureml.core.compute import AmlCompute, ComputeTarget\n",
    "from azureml.data.data_reference import DataReference\n",
    "from azureml.core.runconfig import RunConfiguration, MpiConfiguration\n",
    "from azureml.core import ScriptRunConfig\n",
    "from azureml.train.estimator import Estimator\n",
    "from azureml.exceptions import ComputeTargetException\n",
    "from azureml.widgets import RunDetails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_cluster_name = \"nd12-vnet-clustr\"\n",
    "\n",
    "ws = Workspace.from_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deply the AmlCompute cluster\n",
    "The next cell is deploying the AmlCompute cluster. The cluster is configured to scale down to 0 nodes after 2 minuten, so no cost is incurred while DASK is not running (and thus no nodes are spun up on the cluster as the result of this cell, yet). This cell only needs to be executed once and the cluster can be reused going forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new cluster\n",
      "waiting for nodes\n",
      "Creating\n",
      "Succeeded\n",
      "AmlCompute wait for completion finished\n",
      "Minimum number of nodes requested have been provisioned\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    gpu_cluster = ComputeTarget(workspace=ws, name=gpu_cluster_name)\n",
    "    print('Found existing compute target')\n",
    "    \n",
    "except ComputeTargetException:\n",
    "    print(\"Creating new cluster\")\n",
    "\n",
    "    provisioning_config = AmlCompute.provisioning_configuration(\n",
    "        vm_size=\"Standard_ND12s\", \n",
    "        min_nodes=0, \n",
    "        max_nodes=10,\n",
    "        idle_seconds_before_scaledown=120,\n",
    "        vnet_resourcegroup_name=vnet_resourcegroup_name,\n",
    "        vnet_name=vnet_name,\n",
    "        subnet_name=subnet_name\n",
    "    )\n",
    "    gpu_cluster = ComputeTarget.create(ws, gpu_cluster_name, provisioning_config)\n",
    "\n",
    "    print(\"waiting for nodes\")\n",
    "    gpu_cluster.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copy the data to Azure Blob Storage\n",
    "\n",
    "This next cell is pulling the NYC taxi data set down and then uploads it to the AzureML workspace's default data store. The all nodes of the DASK cluster we are creating further down will then be able to access the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Downloading http://dask-data.s3.amazonaws.com/nyc-taxi/2015/yellow_tripdata_2015-01.csv\n",
      "100%|██████████| 1985964692/1985964692 [00:55<00:00, 35863230.75it/s]\n",
      "- Downloading http://dask-data.s3.amazonaws.com/nyc-taxi/2015/yellow_tripdata_2015-02.csv\n",
      "100%|██████████| 1945357622/1945357622 [00:55<00:00, 34839034.42it/s]\n",
      "- Downloading http://dask-data.s3.amazonaws.com/nyc-taxi/2015/yellow_tripdata_2015-03.csv\n",
      "100%|██████████| 2087971794/2087971794 [00:58<00:00, 35741293.00it/s]\n",
      "- Downloading http://dask-data.s3.amazonaws.com/nyc-taxi/2015/yellow_tripdata_2015-04.csv\n",
      "100%|██████████| 2046225765/2046225765 [00:56<00:00, 35900923.39it/s]\n",
      "- Downloading http://dask-data.s3.amazonaws.com/nyc-taxi/2015/yellow_tripdata_2015-05.csv\n",
      "100%|██████████| 2061869121/2061869121 [00:58<00:00, 35142890.60it/s]\n",
      "- Downloading http://dask-data.s3.amazonaws.com/nyc-taxi/2015/yellow_tripdata_2015-06.csv\n",
      "100%|██████████| 1932049357/1932049357 [00:56<00:00, 34305130.04it/s]\n",
      "- Downloading http://dask-data.s3.amazonaws.com/nyc-taxi/2015/yellow_tripdata_2015-07.csv\n",
      "100%|██████████| 1812530041/1812530041 [00:48<00:00, 37640338.81it/s]\n",
      "- Downloading http://dask-data.s3.amazonaws.com/nyc-taxi/2015/yellow_tripdata_2015-08.csv\n",
      "100%|██████████| 1744852237/1744852237 [00:49<00:00, 35344031.93it/s]\n",
      "- Downloading http://dask-data.s3.amazonaws.com/nyc-taxi/2015/yellow_tripdata_2015-09.csv\n",
      "100%|██████████| 1760412710/1760412710 [00:48<00:00, 36402991.19it/s]\n",
      "- Downloading http://dask-data.s3.amazonaws.com/nyc-taxi/2015/yellow_tripdata_2015-10.csv\n",
      "100%|██████████| 1931460927/1931460927 [00:56<00:00, 34324584.69it/s]\n",
      "- Downloading http://dask-data.s3.amazonaws.com/nyc-taxi/2015/yellow_tripdata_2015-11.csv\n",
      "100%|██████████| 1773468989/1773468989 [00:49<00:00, 36084418.29it/s]\n",
      "- Downloading http://dask-data.s3.amazonaws.com/nyc-taxi/2015/yellow_tripdata_2015-12.csv\n",
      "100%|██████████| 1796283025/1796283025 [00:47<00:00, 37702670.53it/s]\n",
      "- Uploading taxi data... \n",
      "Uploading an estimated of 12 files\n",
      "Uploading /data/home/danielsc/git/azureml-and-dask/azure_taxi/data/nyctaxi/yellow_tripdata_2015-09.csv\n",
      "Uploading /data/home/danielsc/git/azureml-and-dask/azure_taxi/data/nyctaxi/yellow_tripdata_2015-10.csv\n",
      "Uploading /data/home/danielsc/git/azureml-and-dask/azure_taxi/data/nyctaxi/yellow_tripdata_2015-07.csv\n",
      "Uploading /data/home/danielsc/git/azureml-and-dask/azure_taxi/data/nyctaxi/yellow_tripdata_2015-12.csv\n",
      "Uploading /data/home/danielsc/git/azureml-and-dask/azure_taxi/data/nyctaxi/yellow_tripdata_2015-06.csv\n",
      "Uploading /data/home/danielsc/git/azureml-and-dask/azure_taxi/data/nyctaxi/yellow_tripdata_2015-03.csv\n",
      "Uploading /data/home/danielsc/git/azureml-and-dask/azure_taxi/data/nyctaxi/yellow_tripdata_2015-01.csv\n",
      "Uploading /data/home/danielsc/git/azureml-and-dask/azure_taxi/data/nyctaxi/yellow_tripdata_2015-02.csv\n",
      "Uploading /data/home/danielsc/git/azureml-and-dask/azure_taxi/data/nyctaxi/yellow_tripdata_2015-08.csv\n",
      "Uploading /data/home/danielsc/git/azureml-and-dask/azure_taxi/data/nyctaxi/yellow_tripdata_2015-05.csv\n",
      "Uploading /data/home/danielsc/git/azureml-and-dask/azure_taxi/data/nyctaxi/yellow_tripdata_2015-04.csv\n",
      "Uploading /data/home/danielsc/git/azureml-and-dask/azure_taxi/data/nyctaxi/yellow_tripdata_2015-11.csv\n",
      "Uploaded /data/home/danielsc/git/azureml-and-dask/azure_taxi/data/nyctaxi/yellow_tripdata_2015-11.csv, 1 files out of an estimated total of 12\n",
      "Uploaded /data/home/danielsc/git/azureml-and-dask/azure_taxi/data/nyctaxi/yellow_tripdata_2015-12.csv, 2 files out of an estimated total of 12\n",
      "Uploaded /data/home/danielsc/git/azureml-and-dask/azure_taxi/data/nyctaxi/yellow_tripdata_2015-10.csv, 3 files out of an estimated total of 12\n",
      "Uploaded /data/home/danielsc/git/azureml-and-dask/azure_taxi/data/nyctaxi/yellow_tripdata_2015-09.csv, 4 files out of an estimated total of 12\n",
      "Uploaded /data/home/danielsc/git/azureml-and-dask/azure_taxi/data/nyctaxi/yellow_tripdata_2015-08.csv, 5 files out of an estimated total of 12\n",
      "Uploaded /data/home/danielsc/git/azureml-and-dask/azure_taxi/data/nyctaxi/yellow_tripdata_2015-07.csv, 6 files out of an estimated total of 12\n",
      "Uploaded /data/home/danielsc/git/azureml-and-dask/azure_taxi/data/nyctaxi/yellow_tripdata_2015-06.csv, 7 files out of an estimated total of 12\n",
      "Uploaded /data/home/danielsc/git/azureml-and-dask/azure_taxi/data/nyctaxi/yellow_tripdata_2015-02.csv, 8 files out of an estimated total of 12\n",
      "Uploaded /data/home/danielsc/git/azureml-and-dask/azure_taxi/data/nyctaxi/yellow_tripdata_2015-01.csv, 9 files out of an estimated total of 12\n",
      "Uploaded /data/home/danielsc/git/azureml-and-dask/azure_taxi/data/nyctaxi/yellow_tripdata_2015-05.csv, 10 files out of an estimated total of 12\n",
      "Uploaded /data/home/danielsc/git/azureml-and-dask/azure_taxi/data/nyctaxi/yellow_tripdata_2015-04.csv, 11 files out of an estimated total of 12\n",
      "Uploaded /data/home/danielsc/git/azureml-and-dask/azure_taxi/data/nyctaxi/yellow_tripdata_2015-03.csv, 12 files out of an estimated total of 12\n",
      "Uploaded 12 files\n",
      "- Data transfer complete\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import os\n",
    "import sys\n",
    "import urllib.request\n",
    "from tqdm import tqdm\n",
    "from time import sleep\n",
    "\n",
    "cwd = os.getcwd()\n",
    "\n",
    "data_dir = os.path.abspath(os.path.join(cwd, 'data'))\n",
    "if not os.path.exists(data_dir):\n",
    "    os.makedirs(data_dir)\n",
    "\n",
    "taxidir = os.path.join(data_dir, 'nyctaxi')\n",
    "if not os.path.exists(taxidir):\n",
    "    os.makedirs(taxidir)\n",
    "\n",
    "filenames = []\n",
    "local_paths = []\n",
    "for i in range(1, 13):\n",
    "    filename = \"yellow_tripdata_2015-{month:02d}.csv\".format(month=i)\n",
    "    filenames.append(filename)\n",
    "    \n",
    "    local_path = os.path.join(taxidir, filename)\n",
    "    local_paths.append(local_path)\n",
    "\n",
    "for idx, filename in enumerate(filenames):\n",
    "    url = \"http://dask-data.s3.amazonaws.com/nyc-taxi/2015/\" + filename\n",
    "    print(\"- Downloading \" + url)\n",
    "    if not os.path.exists(local_paths[idx]):\n",
    "        with open(local_paths[idx], 'wb') as file:\n",
    "            with urllib.request.urlopen(url) as resp:\n",
    "                length = int(resp.getheader('content-length'))\n",
    "                blocksize = max(4096, length // 100)\n",
    "                with tqdm(total=length, file=sys.stdout) as pbar:\n",
    "                    while True:\n",
    "                        buff = resp.read(blocksize)\n",
    "                        if not buff:\n",
    "                            break\n",
    "                        file.write(buff)\n",
    "                        pbar.update(len(buff))\n",
    "    else:\n",
    "        print(\"- File already exists locally\")\n",
    "\n",
    "print(\"- Uploading taxi data... \")\n",
    "ws = Workspace.from_config()\n",
    "ds = ws.get_default_datastore()\n",
    "\n",
    "ds.upload(\n",
    "    src_dir=taxidir,\n",
    "    target_path='nyctaxi',\n",
    "    show_progress=True)\n",
    "\n",
    "print(\"- Data transfer complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the DASK Cluster\n",
    "\n",
    "On the AMLCompute cluster we are now running a Python job that will run a DASK cluster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - 'gpu_support' is no longer necessary; AzureML now automatically detects and uses nvidia docker extension when it is available. It will be removed in a future release.\n",
      "WARNING - 'gpu_support' is no longer necessary; AzureML now automatically detects and uses nvidia docker extension when it is available. It will be removed in a future release.\n",
      "WARNING - 'gpu_support' is no longer necessary; AzureML now automatically detects and uses nvidia docker extension when it is available. It will be removed in a future release.\n"
     ]
    }
   ],
   "source": [
    "mpi_config = MpiConfiguration()\n",
    "mpi_config.process_count_per_node = 2\n",
    "\n",
    "est = Estimator(\n",
    "    source_directory='./dask',\n",
    "    compute_target=gpu_cluster,\n",
    "    entry_script='init-dask.py',\n",
    "    script_params={\n",
    "        '--data': ws.get_default_datastore(),\n",
    "        '--gpus': str(2)  # The number of GPUs available on each node\n",
    "        },\n",
    "    node_count=3,\n",
    "    use_gpu=True,\n",
    "    distributed_training=mpi_config,\n",
    "    conda_dependencies_file='dask.yml')\n",
    "\n",
    "run = Experiment(ws, \"init-dask-env\").submit(est)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the widget to monitor how the DASK cluster spins up. When run for the first time on a workspace, the following thing will happen:\n",
    "\n",
    "1. The docker image will to be created, which takes about 20 minutes. \n",
    "2. Then AzureML will start to scale the cluster up by provisioning the required number of nodes (`node_count` above), which will take another 5-10 minutes with the chosen Standard_ND12s\n",
    "3. The docker image is being transferred over to the compute nodes, which, given the size of about 8 GB takes another 3-5 minutes\n",
    "\n",
    "So alltogether the process will take up to 30 minutes when run for the first time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76272fefbeca430e81beb71ffc8c86b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "_UserRunWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': False, 'log_level': 'INFO', '…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "RunDetails(run).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "waiting for scheduler node's ip 926\n",
      "headnode has ip:  172.17.0.5\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "import time\n",
    "\n",
    "it = 0\n",
    "while not \"headnode\" in run.get_metrics():\n",
    "    clear_output(wait=True)\n",
    "    print(\"waiting for scheduler node's ip \" + str(it) )\n",
    "    time.sleep(1)\n",
    "    it += 1\n",
    "\n",
    "headnode = run.get_metrics()[\"headnode\"]\n",
    "print(\"headnode has ip: \", headnode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ssh -vvv -N -L 0.0.0.0:8786:{headnode}:8786 -L 0.0.0.0:8787:{headnode}:8787 dask@{nodeip} -p {nodeport}\".format(\n",
    "    headnode=headnode,\n",
    "    nodeip=gpu_cluster.list_nodes()[0]['ipAddress'],\n",
    "    nodeport=gpu_cluster.list_nodes()[0]['port']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import distributed\n",
    "client = distributed.Client('tcp://localhost:8786')\n",
    "client.restart()\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "\n",
    "print(\"- setting dask settings\")\n",
    "dask.config.set({'distributed.scheduler.work-stealing': False})\n",
    "dask.config.set({'distributed.scheduler.bandwidth': 1})\n",
    "\n",
    "print(\"-- Changes to dask settings\")\n",
    "print(\"--- Setting work-stealing to \", dask.config.get('distributed.scheduler.work-stealing'))\n",
    "print(\"--- Setting scheduler bandwidth to \", dask.config.get('distributed.scheduler.bandwidth'))\n",
    "print(\"-- Settings updates complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function which takes a DataFrame partition\n",
    "def clean(df_part, remap, must_haves):    \n",
    "    # some col-names include pre-pended spaces remove & lowercase column names\n",
    "    tmp = {col:col.strip().lower() for col in list(df_part.columns)}\n",
    "    df_part = df_part.rename(tmp)\n",
    "    \n",
    "    # rename using the supplied mapping\n",
    "    df_part = df_part.rename(remap)\n",
    "    \n",
    "    # iterate through columns in this df partition\n",
    "    for col in df_part.columns:\n",
    "        # drop anything not in our expected list\n",
    "        if col not in must_haves:\n",
    "            df_part = df_part.drop(col)\n",
    "            continue\n",
    "        \n",
    "        # if column was read as a string, recast as float\n",
    "        if df_part[col].dtype == 'object':\n",
    "            df_part[col] = df_part[col].str.fillna('-1')\n",
    "            df_part[col] = df_part[col].astype('float32')\n",
    "        else:\n",
    "            # downcast from 64bit to 32bit types\n",
    "            # Tesla T4 are faster on 32bit ops\n",
    "            if 'int' in str(df_part[col].dtype):\n",
    "                df_part[col] = df_part[col].astype('int32')\n",
    "            if 'float' in str(df_part[col].dtype):\n",
    "                df_part[col] = df_part[col].astype('float32')\n",
    "            df_part[col] = df_part[col].fillna(-1)\n",
    "    \n",
    "    return df_part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cudf\n",
    "\n",
    "def read_csv(path):\n",
    "    import cudf\n",
    "    # list of column names that need to be re-mapped\n",
    "    remap = {}\n",
    "    remap['tpep_pickup_datetime'] = 'pickup_datetime'\n",
    "    remap['tpep_dropoff_datetime'] = 'dropoff_datetime'\n",
    "    remap['ratecodeid'] = 'rate_code'\n",
    "\n",
    "    #create a list of columns & dtypes the df must have\n",
    "    must_haves = {\n",
    "     'pickup_datetime': 'datetime64[ms]',\n",
    "     'dropoff_datetime': 'datetime64[ms]',\n",
    "     'passenger_count': 'int32',\n",
    "     'trip_distance': 'float32',\n",
    "     'pickup_longitude': 'float32',\n",
    "     'pickup_latitude': 'float32',\n",
    "     'rate_code': 'int32',\n",
    "     'dropoff_longitude': 'float32',\n",
    "     'dropoff_latitude': 'float32',\n",
    "     'fare_amount': 'float32'\n",
    "    }\n",
    "    \n",
    "    df = cudf.read_csv(path)\n",
    "    return clean(df, remap, must_haves)\n",
    "\n",
    "paths = [os.path.join(run.get_metrics()[\"data\"], \"nyctaxi/\") + filename for filename in filenames]\n",
    "data_paths = client.scatter(paths)\n",
    "dfs = [client.submit(read_csv, data_path) for data_path in data_paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask_cudf\n",
    "\n",
    "taxi_df = dask_cudf.from_delayed(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numba, xgboost, socket\n",
    "import dask, dask_cudf\n",
    "from dask.distributed import Client, wait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply a list of filter conditions to throw out records with missing or outlier values\n",
    "query_frags = [\n",
    "    'fare_amount > 0 and fare_amount < 500',\n",
    "    'passenger_count > 0 and passenger_count < 6',\n",
    "    'pickup_longitude > -75 and pickup_longitude < -73',\n",
    "    'dropoff_longitude > -75 and dropoff_longitude < -73',\n",
    "    'pickup_latitude > 40 and pickup_latitude < 42',\n",
    "    'dropoff_latitude > 40 and dropoff_latitude < 42'\n",
    "]\n",
    "taxi_df = taxi_df.query(' and '.join(query_frags))\n",
    "\n",
    "# inspect the results of cleaning\n",
    "taxi_df.head().to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from math import cos, sin, asin, sqrt, pi\n",
    "import numpy as np\n",
    "\n",
    "def haversine_distance_kernel(pickup_latitude, pickup_longitude, dropoff_latitude, dropoff_longitude, h_distance):\n",
    "    for i, (x_1, y_1, x_2, y_2) in enumerate(zip(pickup_latitude, pickup_longitude, dropoff_latitude, dropoff_longitude)):\n",
    "        x_1 = pi/180 * x_1\n",
    "        y_1 = pi/180 * y_1\n",
    "        x_2 = pi/180 * x_2\n",
    "        y_2 = pi/180 * y_2\n",
    "        \n",
    "        dlon = y_2 - y_1\n",
    "        dlat = x_2 - x_1\n",
    "        a = sin(dlat/2)**2 + cos(x_1) * cos(x_2) * sin(dlon/2)**2\n",
    "        \n",
    "        c = 2 * asin(sqrt(a)) \n",
    "        r = 6371 # Radius of earth in kilometers\n",
    "        \n",
    "        h_distance[i] = c * r\n",
    "\n",
    "def day_of_the_week_kernel(day, month, year, day_of_week):\n",
    "    for i, (d_1, m_1, y_1) in enumerate(zip(day, month, year)):\n",
    "        if month[i] <3:\n",
    "            shift = month[i]\n",
    "        else:\n",
    "            shift = 0\n",
    "        Y = year[i] - (month[i] < 3)\n",
    "        y = Y - 2000\n",
    "        c = 20\n",
    "        d = day[i]\n",
    "        m = month[i] + shift + 1\n",
    "        day_of_week[i] = (d + math.floor(m*2.6) + y + (y//4) + (c//4) -2*c)%7\n",
    "        \n",
    "def add_features(df):\n",
    "    df['hour'] = df['pickup_datetime'].dt.hour\n",
    "    df['year'] = df['pickup_datetime'].dt.year\n",
    "    df['month'] = df['pickup_datetime'].dt.month\n",
    "    df['day'] = df['pickup_datetime'].dt.day\n",
    "    df['diff'] = df['dropoff_datetime'].astype('int32') - df['pickup_datetime'].astype('int32')\n",
    "    \n",
    "    df['pickup_latitude_r'] = df['pickup_latitude']//.01*.01\n",
    "    df['pickup_longitude_r'] = df['pickup_longitude']//.01*.01\n",
    "    df['dropoff_latitude_r'] = df['dropoff_latitude']//.01*.01\n",
    "    df['dropoff_longitude_r'] = df['dropoff_longitude']//.01*.01\n",
    "    \n",
    "    df = df.drop('pickup_datetime')\n",
    "    df = df.drop('dropoff_datetime')\n",
    "    \n",
    "    \n",
    "    df = df.apply_rows(haversine_distance_kernel,\n",
    "                   incols=['pickup_latitude', 'pickup_longitude', 'dropoff_latitude', 'dropoff_longitude'],\n",
    "                   outcols=dict(h_distance=np.float32),\n",
    "                   kwargs=dict())\n",
    "    \n",
    "    \n",
    "    df = df.apply_rows(day_of_the_week_kernel,\n",
    "                      incols=['day', 'month', 'year'],\n",
    "                      outcols=dict(day_of_week=np.float32),\n",
    "                      kwargs=dict())\n",
    "    \n",
    "    \n",
    "    df['is_weekend'] = (df['day_of_week']<2)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# actually add the features\n",
    "taxi_df = taxi_df.map_partitions(add_features).persist()\n",
    "# inspect the result\n",
    "taxi_df.head().to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "taxi_df.groupby('hour').fare_amount.mean().compute().to_pandas().sort_index().plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "X_train = taxi_df.query('day < 25').persist()\n",
    "\n",
    "# create a Y_train ddf with just the target variable\n",
    "Y_train = X_train[['fare_amount']].persist()\n",
    "# drop the target variable from the training ddf\n",
    "X_train = X_train[X_train.columns.difference(['fare_amount'])]\n",
    "\n",
    "# this wont return until all data is in GPU memory\n",
    "done = wait([X_train, Y_train])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes on training with XGBoost with Azure\n",
    "\n",
    "* Because Dask-XGBoost parses the `client` for the raw IP address, it passes `\"localhost\"` to RABIT if the `client` was configured to use `\"localhost\"` with SSH forwarding. This means Dask-XGBoost, as it exists, does not support Azure with this method.\n",
    "* There are several bugs and issues with the Dask submodule of XGBoost:\n",
    "    1. Data co-locality is not enforced (labels and data may not be on the same worker)\n",
    "    2. Data locality is not enforced (a data partition, x, may not be assigned to the worker, n, upon which it resides originally ... so, data may need to be shuffled\n",
    "\n",
    "The latter (Dask submodule of XGBoost) is being fixed in this PR: https://github.com/dmlc/xgboost/pull/4819\n",
    "\n",
    "This means the code below (Dask submodule of XGBoost) will not work, and replacing the call with Dask-XGBoost will not work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "  'num_rounds':   100,\n",
    "  'max_depth':    8,\n",
    "  'max_leaves':   2**8,\n",
    "  'tree_method':  'gpu_hist',\n",
    "  'objective':    'reg:squarederror',\n",
    "  'grow_policy':  'lossguide'\n",
    "}\n",
    "\n",
    "bst = dask_xgboost.train(client, params, X_train, Y_train, num_boost_round=params['num_rounds'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.cancel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function azureml.core.workspace.Workspace.create(name, auth=None, subscription_id=None, resource_group=None, location=None, create_resource_group=True, friendly_name=None, storage_account=None, key_vault=None, app_insights=None, container_registry=None, default_cpu_compute_target=None, default_gpu_compute_target=None, exist_ok=False, show_output=True)>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Workspace.create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (dask)",
   "language": "python",
   "name": "dask"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
