{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "\n",
    "from azureml.core import Workspace, Experiment, Environment\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "from azureml.core.compute import AmlCompute, ComputeTarget\n",
    "from azureml.data.data_reference import DataReference\n",
    "from azureml.core.runconfig import RunConfiguration, MpiConfiguration\n",
    "from azureml.core import ScriptRunConfig\n",
    "from azureml.train.estimator import Estimator\n",
    "from azureml.exceptions import ComputeTargetException\n",
    "from azureml.widgets import RunDetails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_cluster_name = \"nd12-vnet-clustr\"\n",
    "\n",
    "ws = Workspace.from_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new cluster\n",
      "waiting for nodes\n",
      "Creating\n",
      "Succeeded\n",
      "AmlCompute wait for completion finished\n",
      "Minimum number of nodes requested have been provisioned\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    gpu_cluster = ComputeTarget(workspace=ws, name=gpu_cluster_name)\n",
    "    print('Found existing compute target')\n",
    "    \n",
    "except ComputeTargetException:\n",
    "    print(\"Creating new cluster\")\n",
    "\n",
    "    provisioning_config = AmlCompute.provisioning_configuration(\n",
    "        vm_size=\"Standard_ND12s\", \n",
    "        min_nodes=0, \n",
    "        max_nodes=10,\n",
    "        idle_seconds_before_scaledown=120,\n",
    "        vnet_resourcegroup_name='demo',\n",
    "        vnet_name='myvnet',\n",
    "        subnet_name='default'\n",
    "    )\n",
    "    gpu_cluster = ComputeTarget.create(ws, gpu_cluster_name, provisioning_config)\n",
    "\n",
    "    print(\"waiting for nodes\")\n",
    "    gpu_cluster.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Downloading http://dask-data.s3.amazonaws.com/nyc-taxi/2015/yellow_tripdata_2015-01.csv\n",
      "100%|██████████| 1985964692/1985964692 [00:55<00:00, 35863230.75it/s]\n",
      "- Downloading http://dask-data.s3.amazonaws.com/nyc-taxi/2015/yellow_tripdata_2015-02.csv\n",
      "100%|██████████| 1945357622/1945357622 [00:55<00:00, 34839034.42it/s]\n",
      "- Downloading http://dask-data.s3.amazonaws.com/nyc-taxi/2015/yellow_tripdata_2015-03.csv\n",
      "100%|██████████| 2087971794/2087971794 [00:58<00:00, 35741293.00it/s]\n",
      "- Downloading http://dask-data.s3.amazonaws.com/nyc-taxi/2015/yellow_tripdata_2015-04.csv\n",
      "100%|██████████| 2046225765/2046225765 [00:56<00:00, 35900923.39it/s]\n",
      "- Downloading http://dask-data.s3.amazonaws.com/nyc-taxi/2015/yellow_tripdata_2015-05.csv\n",
      "100%|██████████| 2061869121/2061869121 [00:58<00:00, 35142890.60it/s]\n",
      "- Downloading http://dask-data.s3.amazonaws.com/nyc-taxi/2015/yellow_tripdata_2015-06.csv\n",
      "100%|██████████| 1932049357/1932049357 [00:56<00:00, 34305130.04it/s]\n",
      "- Downloading http://dask-data.s3.amazonaws.com/nyc-taxi/2015/yellow_tripdata_2015-07.csv\n",
      "100%|██████████| 1812530041/1812530041 [00:48<00:00, 37640338.81it/s]\n",
      "- Downloading http://dask-data.s3.amazonaws.com/nyc-taxi/2015/yellow_tripdata_2015-08.csv\n",
      "100%|██████████| 1744852237/1744852237 [00:49<00:00, 35344031.93it/s]\n",
      "- Downloading http://dask-data.s3.amazonaws.com/nyc-taxi/2015/yellow_tripdata_2015-09.csv\n",
      "100%|██████████| 1760412710/1760412710 [00:48<00:00, 36402991.19it/s]\n",
      "- Downloading http://dask-data.s3.amazonaws.com/nyc-taxi/2015/yellow_tripdata_2015-10.csv\n",
      "100%|██████████| 1931460927/1931460927 [00:56<00:00, 34324584.69it/s]\n",
      "- Downloading http://dask-data.s3.amazonaws.com/nyc-taxi/2015/yellow_tripdata_2015-11.csv\n",
      "100%|██████████| 1773468989/1773468989 [00:49<00:00, 36084418.29it/s]\n",
      "- Downloading http://dask-data.s3.amazonaws.com/nyc-taxi/2015/yellow_tripdata_2015-12.csv\n",
      "100%|██████████| 1796283025/1796283025 [00:47<00:00, 37702670.53it/s]\n",
      "- Uploading taxi data... \n",
      "Uploading an estimated of 12 files\n",
      "Uploading /data/home/danielsc/git/azureml-and-dask/azure_taxi/data/nyctaxi/yellow_tripdata_2015-09.csv\n",
      "Uploading /data/home/danielsc/git/azureml-and-dask/azure_taxi/data/nyctaxi/yellow_tripdata_2015-10.csv\n",
      "Uploading /data/home/danielsc/git/azureml-and-dask/azure_taxi/data/nyctaxi/yellow_tripdata_2015-07.csv\n",
      "Uploading /data/home/danielsc/git/azureml-and-dask/azure_taxi/data/nyctaxi/yellow_tripdata_2015-12.csv\n",
      "Uploading /data/home/danielsc/git/azureml-and-dask/azure_taxi/data/nyctaxi/yellow_tripdata_2015-06.csv\n",
      "Uploading /data/home/danielsc/git/azureml-and-dask/azure_taxi/data/nyctaxi/yellow_tripdata_2015-03.csv\n",
      "Uploading /data/home/danielsc/git/azureml-and-dask/azure_taxi/data/nyctaxi/yellow_tripdata_2015-01.csv\n",
      "Uploading /data/home/danielsc/git/azureml-and-dask/azure_taxi/data/nyctaxi/yellow_tripdata_2015-02.csv\n",
      "Uploading /data/home/danielsc/git/azureml-and-dask/azure_taxi/data/nyctaxi/yellow_tripdata_2015-08.csv\n",
      "Uploading /data/home/danielsc/git/azureml-and-dask/azure_taxi/data/nyctaxi/yellow_tripdata_2015-05.csv\n",
      "Uploading /data/home/danielsc/git/azureml-and-dask/azure_taxi/data/nyctaxi/yellow_tripdata_2015-04.csv\n",
      "Uploading /data/home/danielsc/git/azureml-and-dask/azure_taxi/data/nyctaxi/yellow_tripdata_2015-11.csv\n",
      "Uploaded /data/home/danielsc/git/azureml-and-dask/azure_taxi/data/nyctaxi/yellow_tripdata_2015-11.csv, 1 files out of an estimated total of 12\n",
      "Uploaded /data/home/danielsc/git/azureml-and-dask/azure_taxi/data/nyctaxi/yellow_tripdata_2015-12.csv, 2 files out of an estimated total of 12\n",
      "Uploaded /data/home/danielsc/git/azureml-and-dask/azure_taxi/data/nyctaxi/yellow_tripdata_2015-10.csv, 3 files out of an estimated total of 12\n",
      "Uploaded /data/home/danielsc/git/azureml-and-dask/azure_taxi/data/nyctaxi/yellow_tripdata_2015-09.csv, 4 files out of an estimated total of 12\n",
      "Uploaded /data/home/danielsc/git/azureml-and-dask/azure_taxi/data/nyctaxi/yellow_tripdata_2015-08.csv, 5 files out of an estimated total of 12\n",
      "Uploaded /data/home/danielsc/git/azureml-and-dask/azure_taxi/data/nyctaxi/yellow_tripdata_2015-07.csv, 6 files out of an estimated total of 12\n",
      "Uploaded /data/home/danielsc/git/azureml-and-dask/azure_taxi/data/nyctaxi/yellow_tripdata_2015-06.csv, 7 files out of an estimated total of 12\n",
      "Uploaded /data/home/danielsc/git/azureml-and-dask/azure_taxi/data/nyctaxi/yellow_tripdata_2015-02.csv, 8 files out of an estimated total of 12\n",
      "Uploaded /data/home/danielsc/git/azureml-and-dask/azure_taxi/data/nyctaxi/yellow_tripdata_2015-01.csv, 9 files out of an estimated total of 12\n",
      "Uploaded /data/home/danielsc/git/azureml-and-dask/azure_taxi/data/nyctaxi/yellow_tripdata_2015-05.csv, 10 files out of an estimated total of 12\n",
      "Uploaded /data/home/danielsc/git/azureml-and-dask/azure_taxi/data/nyctaxi/yellow_tripdata_2015-04.csv, 11 files out of an estimated total of 12\n",
      "Uploaded /data/home/danielsc/git/azureml-and-dask/azure_taxi/data/nyctaxi/yellow_tripdata_2015-03.csv, 12 files out of an estimated total of 12\n",
      "Uploaded 12 files\n",
      "- Data transfer complete\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import os\n",
    "import sys\n",
    "import urllib.request\n",
    "from tqdm import tqdm\n",
    "from time import sleep\n",
    "\n",
    "cwd = os.getcwd()\n",
    "\n",
    "data_dir = os.path.abspath(os.path.join(cwd, 'data'))\n",
    "if not os.path.exists(data_dir):\n",
    "    os.makedirs(data_dir)\n",
    "\n",
    "taxidir = os.path.join(data_dir, 'nyctaxi')\n",
    "if not os.path.exists(taxidir):\n",
    "    os.makedirs(taxidir)\n",
    "\n",
    "filenames = []\n",
    "local_paths = []\n",
    "for i in range(1, 13):\n",
    "    filename = \"yellow_tripdata_2015-{month:02d}.csv\".format(month=i)\n",
    "    filenames.append(filename)\n",
    "    \n",
    "    local_path = os.path.join(taxidir, filename)\n",
    "    local_paths.append(local_path)\n",
    "\n",
    "for idx, filename in enumerate(filenames):\n",
    "    url = \"http://dask-data.s3.amazonaws.com/nyc-taxi/2015/\" + filename\n",
    "    print(\"- Downloading \" + url)\n",
    "    if not os.path.exists(local_paths[idx]):\n",
    "        with open(local_paths[idx], 'wb') as file:\n",
    "            with urllib.request.urlopen(url) as resp:\n",
    "                length = int(resp.getheader('content-length'))\n",
    "                blocksize = max(4096, length // 100)\n",
    "                with tqdm(total=length, file=sys.stdout) as pbar:\n",
    "                    while True:\n",
    "                        buff = resp.read(blocksize)\n",
    "                        if not buff:\n",
    "                            break\n",
    "                        file.write(buff)\n",
    "                        pbar.update(len(buff))\n",
    "    else:\n",
    "        print(\"- File already exists locally\")\n",
    "\n",
    "print(\"- Uploading taxi data... \")\n",
    "ws = Workspace.from_config()\n",
    "ds = ws.get_default_datastore()\n",
    "\n",
    "ds.upload(\n",
    "    src_dir=taxidir,\n",
    "    target_path='nyctaxi',\n",
    "    show_progress=True)\n",
    "\n",
    "print(\"- Data transfer complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - 'gpu_support' is no longer necessary; AzureML now automatically detects and uses nvidia docker extension when it is available. It will be removed in a future release.\n",
      "WARNING - 'gpu_support' is no longer necessary; AzureML now automatically detects and uses nvidia docker extension when it is available. It will be removed in a future release.\n",
      "WARNING - 'gpu_support' is no longer necessary; AzureML now automatically detects and uses nvidia docker extension when it is available. It will be removed in a future release.\n"
     ]
    }
   ],
   "source": [
    "mpi_config = MpiConfiguration()\n",
    "mpi_config.process_count_per_node = 2\n",
    "\n",
    "est = Estimator(\n",
    "    source_directory='./dask',\n",
    "    compute_target=gpu_cluster,\n",
    "    entry_script='init-dask.py',\n",
    "    script_params={\n",
    "        '--data': ws.get_default_datastore(),\n",
    "        '--gpus': str(2)  # The number of GPUs available on each node\n",
    "        },\n",
    "    node_count=3,\n",
    "    use_gpu=True,\n",
    "    distributed_training=mpi_config,\n",
    "    conda_dependencies_file='dask.yml')\n",
    "\n",
    "run = Experiment(ws, \"init-dask-env\").submit(est)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"width:100%\"><tr><th>Experiment</th><th>Id</th><th>Type</th><th>Status</th><th>Details Page</th><th>Docs Page</th></tr><tr><td>init-dask-env</td><td>init-dask-env_1569331853_912b4494</td><td>azureml.scriptrun</td><td>Starting</td><td><a href=\"https://mlworkspace.azure.ai/portal/subscriptions/4feb84f6-2c10-4536-9c8a-0a2360eabfc5/resourceGroups/demo/providers/Microsoft.MachineLearningServices/workspaces/vnettest/experiments/init-dask-env/runs/init-dask-env_1569331853_912b4494\" target=\"_blank\" rel=\"noopener\">Link to Azure Portal</a></td><td><a href=\"https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.script_run.ScriptRun?view=azure-ml-py\" target=\"_blank\" rel=\"noopener\">Link to Documentation</a></td></tr></table>"
      ],
      "text/plain": [
       "Run(Experiment: init-dask-env,\n",
       "Id: init-dask-env_1569331853_912b4494,\n",
       "Type: azureml.scriptrun,\n",
       "Status: Starting)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "waiting for scheduler node's ip 926\n",
      "headnode has ip:  172.17.0.5\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "import time\n",
    "\n",
    "it = 0\n",
    "while not \"headnode\" in run.get_metrics():\n",
    "    clear_output(wait=True)\n",
    "    print(\"waiting for scheduler node's ip \" + str(it) )\n",
    "    time.sleep(1)\n",
    "    it += 1\n",
    "\n",
    "headnode = run.get_metrics()[\"headnode\"]\n",
    "print(\"headnode has ip: \", headnode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ssh -vvv -N -L 0.0.0.0:8786:{headnode}:8786 -L 0.0.0.0:8787:{headnode}:8787 dask@{nodeip} -p {nodeport}\".format(\n",
    "    headnode=headnode,\n",
    "    nodeip=gpu_cluster.list_nodes()[0]['ipAddress'],\n",
    "    nodeport=gpu_cluster.list_nodes()[0]['port']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import distributed\n",
    "client = distributed.Client('tcp://localhost:8786')\n",
    "client.restart()\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "\n",
    "print(\"- setting dask settings\")\n",
    "dask.config.set({'distributed.scheduler.work-stealing': False})\n",
    "dask.config.set({'distributed.scheduler.bandwidth': 1})\n",
    "\n",
    "print(\"-- Changes to dask settings\")\n",
    "print(\"--- Setting work-stealing to \", dask.config.get('distributed.scheduler.work-stealing'))\n",
    "print(\"--- Setting scheduler bandwidth to \", dask.config.get('distributed.scheduler.bandwidth'))\n",
    "print(\"-- Settings updates complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function which takes a DataFrame partition\n",
    "def clean(df_part, remap, must_haves):    \n",
    "    # some col-names include pre-pended spaces remove & lowercase column names\n",
    "    tmp = {col:col.strip().lower() for col in list(df_part.columns)}\n",
    "    df_part = df_part.rename(tmp)\n",
    "    \n",
    "    # rename using the supplied mapping\n",
    "    df_part = df_part.rename(remap)\n",
    "    \n",
    "    # iterate through columns in this df partition\n",
    "    for col in df_part.columns:\n",
    "        # drop anything not in our expected list\n",
    "        if col not in must_haves:\n",
    "            df_part = df_part.drop(col)\n",
    "            continue\n",
    "        \n",
    "        # if column was read as a string, recast as float\n",
    "        if df_part[col].dtype == 'object':\n",
    "            df_part[col] = df_part[col].str.fillna('-1')\n",
    "            df_part[col] = df_part[col].astype('float32')\n",
    "        else:\n",
    "            # downcast from 64bit to 32bit types\n",
    "            # Tesla T4 are faster on 32bit ops\n",
    "            if 'int' in str(df_part[col].dtype):\n",
    "                df_part[col] = df_part[col].astype('int32')\n",
    "            if 'float' in str(df_part[col].dtype):\n",
    "                df_part[col] = df_part[col].astype('float32')\n",
    "            df_part[col] = df_part[col].fillna(-1)\n",
    "    \n",
    "    return df_part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cudf\n",
    "\n",
    "def read_csv(path):\n",
    "    import cudf\n",
    "    # list of column names that need to be re-mapped\n",
    "    remap = {}\n",
    "    remap['tpep_pickup_datetime'] = 'pickup_datetime'\n",
    "    remap['tpep_dropoff_datetime'] = 'dropoff_datetime'\n",
    "    remap['ratecodeid'] = 'rate_code'\n",
    "\n",
    "    #create a list of columns & dtypes the df must have\n",
    "    must_haves = {\n",
    "     'pickup_datetime': 'datetime64[ms]',\n",
    "     'dropoff_datetime': 'datetime64[ms]',\n",
    "     'passenger_count': 'int32',\n",
    "     'trip_distance': 'float32',\n",
    "     'pickup_longitude': 'float32',\n",
    "     'pickup_latitude': 'float32',\n",
    "     'rate_code': 'int32',\n",
    "     'dropoff_longitude': 'float32',\n",
    "     'dropoff_latitude': 'float32',\n",
    "     'fare_amount': 'float32'\n",
    "    }\n",
    "    \n",
    "    df = cudf.read_csv(path)\n",
    "    return clean(df, remap, must_haves)\n",
    "\n",
    "paths = [os.path.join(run.get_metrics()[\"data\"], \"nyctaxi/\") + filename for filename in filenames]\n",
    "data_paths = client.scatter(paths)\n",
    "dfs = [client.submit(read_csv, data_path) for data_path in data_paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask_cudf\n",
    "\n",
    "taxi_df = dask_cudf.from_delayed(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numba, xgboost, socket\n",
    "import dask, dask_cudf\n",
    "from dask.distributed import Client, wait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply a list of filter conditions to throw out records with missing or outlier values\n",
    "query_frags = [\n",
    "    'fare_amount > 0 and fare_amount < 500',\n",
    "    'passenger_count > 0 and passenger_count < 6',\n",
    "    'pickup_longitude > -75 and pickup_longitude < -73',\n",
    "    'dropoff_longitude > -75 and dropoff_longitude < -73',\n",
    "    'pickup_latitude > 40 and pickup_latitude < 42',\n",
    "    'dropoff_latitude > 40 and dropoff_latitude < 42'\n",
    "]\n",
    "taxi_df = taxi_df.query(' and '.join(query_frags))\n",
    "\n",
    "# inspect the results of cleaning\n",
    "taxi_df.head().to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from math import cos, sin, asin, sqrt, pi\n",
    "import numpy as np\n",
    "\n",
    "def haversine_distance_kernel(pickup_latitude, pickup_longitude, dropoff_latitude, dropoff_longitude, h_distance):\n",
    "    for i, (x_1, y_1, x_2, y_2) in enumerate(zip(pickup_latitude, pickup_longitude, dropoff_latitude, dropoff_longitude)):\n",
    "        x_1 = pi/180 * x_1\n",
    "        y_1 = pi/180 * y_1\n",
    "        x_2 = pi/180 * x_2\n",
    "        y_2 = pi/180 * y_2\n",
    "        \n",
    "        dlon = y_2 - y_1\n",
    "        dlat = x_2 - x_1\n",
    "        a = sin(dlat/2)**2 + cos(x_1) * cos(x_2) * sin(dlon/2)**2\n",
    "        \n",
    "        c = 2 * asin(sqrt(a)) \n",
    "        r = 6371 # Radius of earth in kilometers\n",
    "        \n",
    "        h_distance[i] = c * r\n",
    "\n",
    "def day_of_the_week_kernel(day, month, year, day_of_week):\n",
    "    for i, (d_1, m_1, y_1) in enumerate(zip(day, month, year)):\n",
    "        if month[i] <3:\n",
    "            shift = month[i]\n",
    "        else:\n",
    "            shift = 0\n",
    "        Y = year[i] - (month[i] < 3)\n",
    "        y = Y - 2000\n",
    "        c = 20\n",
    "        d = day[i]\n",
    "        m = month[i] + shift + 1\n",
    "        day_of_week[i] = (d + math.floor(m*2.6) + y + (y//4) + (c//4) -2*c)%7\n",
    "        \n",
    "def add_features(df):\n",
    "    df['hour'] = df['pickup_datetime'].dt.hour\n",
    "    df['year'] = df['pickup_datetime'].dt.year\n",
    "    df['month'] = df['pickup_datetime'].dt.month\n",
    "    df['day'] = df['pickup_datetime'].dt.day\n",
    "    df['diff'] = df['dropoff_datetime'].astype('int32') - df['pickup_datetime'].astype('int32')\n",
    "    \n",
    "    df['pickup_latitude_r'] = df['pickup_latitude']//.01*.01\n",
    "    df['pickup_longitude_r'] = df['pickup_longitude']//.01*.01\n",
    "    df['dropoff_latitude_r'] = df['dropoff_latitude']//.01*.01\n",
    "    df['dropoff_longitude_r'] = df['dropoff_longitude']//.01*.01\n",
    "    \n",
    "    df = df.drop('pickup_datetime')\n",
    "    df = df.drop('dropoff_datetime')\n",
    "    \n",
    "    \n",
    "    df = df.apply_rows(haversine_distance_kernel,\n",
    "                   incols=['pickup_latitude', 'pickup_longitude', 'dropoff_latitude', 'dropoff_longitude'],\n",
    "                   outcols=dict(h_distance=np.float32),\n",
    "                   kwargs=dict())\n",
    "    \n",
    "    \n",
    "    df = df.apply_rows(day_of_the_week_kernel,\n",
    "                      incols=['day', 'month', 'year'],\n",
    "                      outcols=dict(day_of_week=np.float32),\n",
    "                      kwargs=dict())\n",
    "    \n",
    "    \n",
    "    df['is_weekend'] = (df['day_of_week']<2)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# actually add the features\n",
    "taxi_df = taxi_df.map_partitions(add_features).persist()\n",
    "# inspect the result\n",
    "taxi_df.head().to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "taxi_df.groupby('hour').fare_amount.mean().compute().to_pandas().sort_index().plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "X_train = taxi_df.query('day < 25').persist()\n",
    "\n",
    "# create a Y_train ddf with just the target variable\n",
    "Y_train = X_train[['fare_amount']].persist()\n",
    "# drop the target variable from the training ddf\n",
    "X_train = X_train[X_train.columns.difference(['fare_amount'])]\n",
    "\n",
    "# this wont return until all data is in GPU memory\n",
    "done = wait([X_train, Y_train])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes on training with XGBoost with Azure\n",
    "\n",
    "* Because Dask-XGBoost parses the `client` for the raw IP address, it passes `\"localhost\"` to RABIT if the `client` was configured to use `\"localhost\"` with SSH forwarding. This means Dask-XGBoost, as it exists, does not support Azure with this method.\n",
    "* There are several bugs and issues with the Dask submodule of XGBoost:\n",
    "    1. Data co-locality is not enforced (labels and data may not be on the same worker)\n",
    "    2. Data locality is not enforced (a data partition, x, may not be assigned to the worker, n, upon which it resides originally ... so, data may need to be shuffled\n",
    "\n",
    "The latter (Dask submodule of XGBoost) is being fixed in this PR: https://github.com/dmlc/xgboost/pull/4819\n",
    "\n",
    "This means the code below (Dask submodule of XGBoost) will not work, and replacing the call with Dask-XGBoost will not work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "  'num_rounds':   100,\n",
    "  'max_depth':    8,\n",
    "  'max_leaves':   2**8,\n",
    "  'tree_method':  'gpu_hist',\n",
    "  'objective':    'reg:squarederror',\n",
    "  'grow_policy':  'lossguide'\n",
    "}\n",
    "\n",
    "bst = dask_xgboost.train(client, params, X_train, Y_train, num_boost_round=params['num_rounds'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (dask)",
   "language": "python",
   "name": "dask"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
