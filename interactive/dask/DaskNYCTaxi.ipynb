{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallelize Pandas with Dask.dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "from dask.distributed import Client, progress\n",
    "from dask import delayed\n",
    "df = None\n",
    "c = Client('tcp://localhost:8786')\n",
    "c.restart()\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace, Run\n",
    "import os\n",
    "run = Run.get_context()\n",
    "ws = run.experiment.workspace\n",
    "\n",
    "## or load directly through blob file system\n",
    "# using https://github.com/dask/adlfs -- still pretty beta, \n",
    "# throws an error message, but seesm to work\n",
    "ds = ws.get_default_datastore()\n",
    "ACCOUNT_NAME = ds.account_name\n",
    "ACCOUNT_KEY = ds.account_key\n",
    "CONTAINER = ds.container_name\n",
    "import dask.dataframe as dd\n",
    "from fsspec.registry import known_implementations\n",
    "known_implementations['abfs'] = {'class': 'adlfs.AzureBlobFileSystem'}\n",
    "STORAGE_OPTIONS={'account_name': ACCOUNT_NAME, 'account_key': ACCOUNT_KEY}\n",
    "df = dd.read_csv(f'abfs://{CONTAINER}/nyctaxi/2015/*.csv', \n",
    "                 storage_options=STORAGE_OPTIONS,\n",
    "                 parse_dates=['tpep_pickup_datetime', 'tpep_dropoff_datetime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enable this code path instead of the above if you run into\n",
    "# any issues with the AzureBlobFileSystem (https://github.com/dask/adlfs)\n",
    "# this will load the data from the workspace blob storage mounted via blobFUSE\n",
    "if False:\n",
    "    from azureml.core import Workspace\n",
    "    ## get the last run on the dask experiment which should be running \n",
    "    ## our dask cluster, and retrieve the data path from it\n",
    "    ws = Workspace.from_config()\n",
    "    exp = ws.experiments['dask']\n",
    "    run = None\n",
    "    for run in ws.experiments['dask'].get_runs():\n",
    "        if run.get_status() == \"Running\":\n",
    "            cluster_run = run\n",
    "            break;\n",
    "\n",
    "    if (run == None):\n",
    "        raise Exception('Cluster should be in state \\'Running\\'')\n",
    "\n",
    "    data_path = cluster_run.get_metrics()['datastore'] + '/nyctaxi'\n",
    "\n",
    "\n",
    "    import dask\n",
    "    import dask.dataframe as dd\n",
    "    from dask import delayed\n",
    "\n",
    "    def load_data(path):\n",
    "        return dd.read_csv(path, parse_dates=['tpep_pickup_datetime', 'tpep_dropoff_datetime'])\n",
    "\n",
    "    data_2015 = data_path + '/2015'\n",
    "    data_2015_csv = data_2015 + '/*.csv'\n",
    "    df = delayed(load_data)(data_2015_csv).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fall back to this path if neither of the above paths have been enabled\n",
    "if df is None:\n",
    "    ## or in this case straight from GOOGLE Storage\n",
    "    import dask.dataframe as dd\n",
    "    df = dd.read_csv('gcs://anaconda-public-data/nyc-taxi/csv/2015/yellow_*.csv',\n",
    "                     storage_options={'token': 'anon'}, \n",
    "                     parse_dates=['tpep_pickup_datetime', 'tpep_dropoff_datetime'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%time len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time df.map_partitions(len).compute().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Dask DataFrames\n",
    "---------------\n",
    "\n",
    "*  Coordinate many Pandas DataFrames across a cluster\n",
    "*  Faithfully implement a subset of the Pandas API\n",
    "*  Use Pandas under the hood (for speed and maturity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of column names that need to be re-mapped\n",
    "remap = {}\n",
    "remap['tpep_pickup_datetime'] = 'pickup_datetime'\n",
    "remap['tpep_dropoff_datetime'] = 'dropoff_datetime'\n",
    "remap['RatecodeID'] = 'rate_code'\n",
    "\n",
    "#create a list of columns & dtypes the df must have\n",
    "must_haves = {\n",
    "    'VendorID': 'object',\n",
    "    'pickup_datetime': 'datetime64[ms]',\n",
    "    'dropoff_datetime': 'datetime64[ms]',\n",
    "    'passenger_count': 'int32',\n",
    "    'trip_distance': 'float32',\n",
    "    'pickup_longitude': 'float32',\n",
    "    'pickup_latitude': 'float32',\n",
    "    'rate_code': 'int32',\n",
    "    'payment_type': 'int32',\n",
    "    'dropoff_longitude': 'float32',\n",
    "    'dropoff_latitude': 'float32',\n",
    "    'fare_amount': 'float32',\n",
    "    'tip_amount': 'float32',\n",
    "    'total_amount': 'float32'\n",
    "}\n",
    "\n",
    "query_frags = [\n",
    "    'fare_amount > 0 and fare_amount < 500',\n",
    "    'passenger_count > 0 and passenger_count < 6',\n",
    "    'pickup_longitude > -75 and pickup_longitude < -73',\n",
    "    'dropoff_longitude > -75 and dropoff_longitude < -73',\n",
    "    'pickup_latitude > 40 and pickup_latitude < 42',\n",
    "    'dropoff_latitude > 40 and dropoff_latitude < 42'\n",
    "]\n",
    "query = ' and '.join(query_frags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function which takes a DataFrame partition\n",
    "def clean(df_part, remap, must_haves, query):    \n",
    "    df_part = df_part.query(query)\n",
    "    \n",
    "    # some col-names include pre-pended spaces remove & lowercase column names\n",
    "    # tmp = {col:col.strip().lower() for col in list(df_part.columns)}\n",
    "\n",
    "    # rename using the supplied mapping\n",
    "    df_part = df_part.rename(columns=remap)\n",
    "    \n",
    "    # iterate through columns in this df partition\n",
    "    for col in df_part.columns:\n",
    "        # drop anything not in our expected list\n",
    "        if col not in must_haves:\n",
    "            df_part = df_part.drop(col, axis=1)\n",
    "            continue\n",
    "\n",
    "        if df_part[col].dtype == 'object' and col in ['pickup_datetime', 'dropoff_datetime']:\n",
    "            df_part[col] = df_part[col].astype('datetime64[ms]')\n",
    "            continue\n",
    "            \n",
    "        # if column was read as a string, recast as float\n",
    "        if df_part[col].dtype == 'object':\n",
    "            df_part[col] = df_part[col].str.fillna('-1')\n",
    "            df_part[col] = df_part[col].astype('float32')\n",
    "        else:\n",
    "            # save some memory by using 32 bit floats\n",
    "            if 'int' in str(df_part[col].dtype):\n",
    "                df_part[col] = df_part[col].astype('int32')\n",
    "            if 'float' in str(df_part[col].dtype):\n",
    "                df_part[col] = df_part[col].astype('float32')\n",
    "            df_part[col] = df_part[col].fillna(-1)\n",
    "    \n",
    "    return df_part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_df = clean(df, remap, must_haves, query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from math import  pi\n",
    "from dask.array import cos, sin, arcsin, sqrt, floor\n",
    "import numpy as np\n",
    "\n",
    "def haversine_distance(pickup_latitude, pickup_longitude, dropoff_latitude, dropoff_longitude):\n",
    "    x_1 = pi / 180 * pickup_latitude\n",
    "    y_1 = pi / 180 * pickup_longitude\n",
    "    x_2 = pi / 180 * dropoff_latitude\n",
    "    y_2 = pi / 180 * dropoff_longitude\n",
    "\n",
    "    dlon = y_2 - y_1\n",
    "    dlat = x_2 - x_1\n",
    "    a = sin(dlat / 2)**2 + cos(x_1) * cos(x_2) * sin(dlon / 2)**2\n",
    "\n",
    "    c = 2 * arcsin(sqrt(a)) \n",
    "    r = 6371 # Radius of earth in kilometers\n",
    "\n",
    "    return c * r\n",
    "\n",
    "def day_of_the_week(day, month, year):\n",
    "    if month < 3:\n",
    "        shift = month\n",
    "    else:\n",
    "        shift = 0\n",
    "    Y = year - (month < 3)\n",
    "    y = Y - 2000\n",
    "    c = 20\n",
    "    d = day\n",
    "    m = month + shift + 1\n",
    "    return (d + floor(m * 2.6) + y + (y // 4) + (c // 4) - 2 * c) % 7\n",
    "        \n",
    "def add_features(df):\n",
    "    df['hour'] = df['pickup_datetime'].dt.hour.astype('int32')\n",
    "    df['year'] = df['pickup_datetime'].dt.year.astype('int32')\n",
    "    df['month'] = df['pickup_datetime'].dt.month.astype('int32')\n",
    "    df['day'] = df['pickup_datetime'].dt.day.astype('int32')\n",
    "    df['day_of_week'] = df['pickup_datetime'].dt.weekday.astype('int32')\n",
    "       \n",
    "    #df['diff'] = df['dropoff_datetime'].astype('int32') - df['pickup_datetime'].astype('int32')\n",
    "    df['diff'] = df['dropoff_datetime'] - df['pickup_datetime']\n",
    "    \n",
    "    df['pickup_latitude_r'] = (df['pickup_latitude'] // .01 * .01).astype('float32')\n",
    "    df['pickup_longitude_r'] = (df['pickup_longitude'] // .01 * .01).astype('float32')\n",
    "    df['dropoff_latitude_r'] = (df['dropoff_latitude'] // .01 * .01).astype('float32')\n",
    "    df['dropoff_longitude_r'] = (df['dropoff_longitude'] // .01 * .01).astype('float32')\n",
    "    \n",
    "    #df = df.drop('pickup_datetime', axis=1)\n",
    "    #df = df.drop('dropoff_datetime', axis=1)\n",
    "\n",
    "    #df = df.apply_rows(haversine_distance_kernel,\n",
    "    #                   incols=['pickup_latitude', 'pickup_longitude', 'dropoff_latitude', 'dropoff_longitude'],\n",
    "    #                   outcols=dict(h_distance=np.float32),\n",
    "    #                   kwargs=dict())\n",
    "\n",
    "    import numpy\n",
    "\n",
    "    df['h_distance'] = haversine_distance(df['pickup_latitude'], \n",
    "                                          df['pickup_longitude'], \n",
    "                                          df['dropoff_latitude'], \n",
    "                                          df['dropoff_longitude']).astype('float32')\n",
    "\n",
    "    #df = df.apply_rows(day_of_the_week_kernel,\n",
    "    #                   incols=['day', 'month', 'year'],\n",
    "    #                   outcols=dict(day_of_week=np.float32),\n",
    "    #                   kwargs=dict())\n",
    "    #df['day_of_week'] = numpy.empty(len(df), dtype=np.int32)\n",
    "    #day_of_the_week_kernel(df['day'],\n",
    "    #                       df['month'],\n",
    "    #                       df['year'],\n",
    "    #                       df['day_of_week'])\n",
    "    \n",
    "    \n",
    "    df['is_weekend'] = (df['day_of_week']>5).astype(\"int32\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_df = add_features(taxi_df)\n",
    "taxi_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_df = taxi_df.persist()\n",
    "progress(taxi_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time len(taxi_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time taxi_df.passenger_count.sum().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute average trip distance grouped by passenger count\n",
    "taxi_df.groupby('passenger_count').trip_distance.mean().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tip Fraction, grouped by day-of-week and hour-of-day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = taxi_df[(taxi_df.tip_amount > 0) & (taxi_df.fare_amount > 0)]\n",
    "df2['tip_fraction'] = df2.tip_amount / df2.fare_amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group df.tpep_pickup_datetime by dayofweek and hour\n",
    "dayofweek = df2.groupby(df2.pickup_datetime.dt.dayofweek).tip_fraction.mean() \n",
    "hour = df2.groupby(df2.pickup_datetime.dt.hour).tip_fraction.mean()\n",
    "\n",
    "dayofweek, hour = dask.persist(dayofweek, hour)\n",
    "progress(dayofweek, hour)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot results\n",
    "\n",
    "This requires matplotlib to be installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hour.compute().plot(figsize=(10, 6), title='Tip Fraction by Hour')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dayofweek.compute().plot(figsize=(10, 6), title='Tip Fraction by Day of Week')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "taxi_df.groupby('passenger_count').fare_amount.mean().compute().sort_index().plot(legend=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_df.groupby(taxi_df.passenger_count).trip_distance.mean().compute().plot(legend=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "by_payment = taxi_df.groupby(taxi_df.payment_type).fare_amount.count().compute()\n",
    "by_payment.index = by_payment.index.map({1: 'Credit card',\n",
    "    2: 'Cash',\n",
    "    3: 'No charge',\n",
    "    4: 'Dispute',\n",
    "    5: 'Unknown',\n",
    "    6: 'Voided trip'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "by_payment.plot(legend=True, kind='bar')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's save the transformed dataset back to blob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "output_uuid = uuid.uuid1().hex\n",
    "run.log('output_uuid', output_uuid)\n",
    "\n",
    "output_path = run.get_metrics()['datastore'] + '/output/' + output_uuid + '.parquet'\n",
    "\n",
    "print('save parquet to ', output_path)\n",
    "\n",
    "taxi_df.to_parquet(output_path)\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import dask\n",
    "import dask.dataframe as dd\n",
    "\n",
    "df = dd.read_parquet(output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (dask)",
   "language": "python",
   "name": "dask"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
