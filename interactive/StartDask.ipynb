{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Dask on AzureML\n",
    "\n",
    "This notebook shows how to run a Dask cluster on an AzureML Compute cluster. \n",
    "For setup instructions, please see the [Readme](README.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting the cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.0.41'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from azureml.core import Workspace, Experiment\n",
    "from azureml.train.estimator import Estimator\n",
    "from azureml.widgets import RunDetails\n",
    "from azureml.core.runconfig import MpiConfiguration\n",
    "from azureml.core import VERSION\n",
    "import time\n",
    "VERSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will get the workspace and AML compute cluster and start the Dask cluster on it. The assumption is that you have created a cluster with the name `dask` -- else change the name below accordingly. **It is important that, as you created the cluster, you have provided a username (I am using `daskuser`) and password and ssh key (ssh key is optional), since you will need to log in to the worker nodes to establish the port forwarding to the docker container.**\n",
    "\n",
    "![create_cluster](img/create_cluster.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = Workspace.from_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_cluster = ws.compute_targets['dask']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting the Dask cluster using an Estimator with MpiConfiguration. Make sure the cluster is able to scale up to 10 nodes or change the `node_count` below. Also, this example is launching 2 workers per node (assuming 2 core machines). If you are running on more cores, you can change the `process_count_per_node` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpi_configuration = MpiConfiguration()\n",
    "mpi_configuration.process_count_per_node = 2\n",
    "\n",
    "est = Estimator('dask', \n",
    "                compute_target=dask_cluster, \n",
    "                entry_script='startDask.py', \n",
    "                conda_dependencies_file_path='environment.yml', \n",
    "                script_params={'--data': ws.get_default_datastore()},\n",
    "                node_count=10,\n",
    "                distributed_training=mpi_configuration)\n",
    "\n",
    "run = Experiment(ws, 'dask').submit(est)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8dc9f6f60e904ad987c881af16d1c82a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "_UserRunWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': False, 'log_level': 'INFO', 'â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "RunDetails(run).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Headnode has IP: 10.0.0.4\n"
     ]
    }
   ],
   "source": [
    "while not 'headnode' in run.get_metrics():\n",
    "    print(\"waiting for scheduler node's ip\")\n",
    "    time.sleep(5)\n",
    "\n",
    "print('Headnode has IP:', run.get_metrics()['headnode'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Establish the port-forwarding from Notebook VM to Dask Scheduler\n",
    "Since Notebook VM does not yet support VNets, you need to build an SSH port forwarder through SSH login.\n",
    "\n",
    "First you need to find the IP and port of one of the nodes on the cluster by going to the nodes tab of the cluster. Note down the IP and port -- in my case it is `52.137.63.123` and port `50000`.\n",
    "\n",
    "![compute_nodes](img/compute_nodes.png)\n",
    "\n",
    "Then, open the terminal on the Notebook VM and type the following:  \n",
    "`ssh daskuser@<clusternode IP> -p <clusternode port> -L 8786:<headnode IP>:8786 -L 8787:<headnode IP>:8787`\n",
    "\n",
    "In my case it is:\n",
    "`ssh daskuser@52.137.63.123 -p 50000 -L 8786:10.0.0.4:8786 -L 8787:10.0.0.4:8787`\n",
    "\n",
    "Make sure to leave the terminal tab open to keep the port-forward running\n",
    "\n",
    "As you see, you are forwarding 2 ports 8786 is for the scheduler and 8787 is for the Bokeh app that shows the activity on the cluster. To access the Bokeh app, change the URL to your notebook VM by adding `-8787` right after the machine name. In my case it looks like this:\n",
    "\n",
    "https://danielsctest-8787.westeurope.notebooks.azureml.net/\n",
    "\n",
    "Hopefully, you are seeing this after you clicked on 'Status':\n",
    "\n",
    "![Bokeh](img/bokeh.png)\n",
    "\n",
    "If you are wondering what all this port business in accomplishing, please see the graph below that tries to illustrate who talks to whom and how.\n",
    "\n",
    "![Network](img/network.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run some jobs on the cluster\n",
    "If you are able to see the Bokeh app, it is time to use the cluster. Thanks to the port forward, the scheduler appears to the notebook VM at `tcp://localhost:8786`. You should see 19 workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3>Client</h3>\n",
       "<ul>\n",
       "  <li><b>Scheduler: </b>tcp://localhost:8786\n",
       "  <li><b>Dashboard: </b><a href='http://localhost:8787/status' target='_blank'>http://localhost:8787/status</a>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3>Cluster</h3>\n",
       "<ul>\n",
       "  <li><b>Workers: </b>19</li>\n",
       "  <li><b>Cores: </b>38</li>\n",
       "  <li><b>Memory: </b>138.86 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: scheduler='tcp://10.0.0.4:8786' processes=19 cores=38>"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dask.distributed import Client\n",
    "\n",
    "c = Client('tcp://localhost:8786')\n",
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See if the cluster works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>Future: inc</b> <font color=\"gray\">status: </font><font color=\"black\">pending</font>, <font color=\"gray\">key: </font>inc-b16a3e98763b4bf292b5d6fb6aa9d853"
      ],
      "text/plain": [
       "<Future: status: pending, key: inc-b16a3e98763b4bf292b5d6fb6aa9d853>"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def inc(x):\n",
    "    return x + 1\n",
    "\n",
    "fut = c.submit(inc, 1)\n",
    "fut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fut.result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training on Large Datasets\n",
    "(from https://github.com/dask/dask-tutorial)\n",
    "\n",
    "Sometimes you'll want to train on a larger than memory dataset. `dask-ml` has implemented estimators that work well on dask arrays and dataframes that may be larger than your machine's RAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "import joblib\n",
    "import dask.array as da\n",
    "import dask.delayed\n",
    "from sklearn.datasets import make_blobs\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll make a small (random) dataset locally using scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.00796679,  4.34582168,  2.15175661,  1.04337835, -1.82115164,\n",
       "         2.81149666, -1.18757701,  7.74628882,  9.36761449, -2.20570731,\n",
       "         5.71142324,  0.41084221,  1.34168817,  8.4568751 , -8.59042755,\n",
       "        -8.35194302, -9.55383028,  6.68605157,  5.34481483,  7.35044606],\n",
       "       [ 9.49283024,  6.1422784 , -0.97484846,  5.8604399 , -7.61126963,\n",
       "         2.86555735, -7.25390288,  8.89609285,  0.33510318, -1.79181328,\n",
       "        -4.66192239,  5.43323887, -0.86162507,  1.3705568 , -9.7904172 ,\n",
       "         2.3613231 ,  2.20516237,  2.20604823,  8.76464833,  3.47795068],\n",
       "       [-2.67206588, -1.30103177,  3.98418492, -8.88040428,  3.27735964,\n",
       "         3.51616445, -5.81395151, -7.42287114, -3.73476887, -2.89520363,\n",
       "         1.49435043, -1.35811028,  9.91250767, -7.86133474, -5.78975793,\n",
       "        -6.54897163,  3.08083281, -5.18975209, -0.85563107, -5.06615534],\n",
       "       [-6.85980599, -7.87144648,  3.33572279, -7.00394241, -5.97224874,\n",
       "        -2.55638942,  6.36329802, -7.97988653,  6.80059611, -8.14552537,\n",
       "         9.48255539, -0.67232114,  9.38462699,  2.09067352,  4.80505419,\n",
       "        -9.14866204, -4.32240399, -7.61670696, -4.14166466, -7.73998277]])"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_centers = 12\n",
    "n_features = 20\n",
    "\n",
    "X_small, y_small = make_blobs(n_samples=1000, centers=n_centers, n_features=n_features, random_state=0)\n",
    "\n",
    "centers = np.zeros((n_centers, n_features))\n",
    "\n",
    "for i in range(n_centers):\n",
    "    centers[i] = X_small[y_small == i].mean(0)\n",
    "    \n",
    "centers[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The small dataset will be the template for our large random dataset.\n",
    "We'll use `dask.delayed` to adapt `sklearn.datasets.make_blobs`, so that the actual dataset is being generated on our workers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dask.array<concatenate, shape=(100000000, 20), dtype=float64, chunksize=(200000, 20)>"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_samples_per_block = 200000\n",
    "n_blocks = 500\n",
    "\n",
    "delayeds = [dask.delayed(make_blobs)(n_samples=n_samples_per_block,\n",
    "                                     centers=centers,\n",
    "                                     n_features=n_features,\n",
    "                                     random_state=i)[0]\n",
    "            for i in range(n_blocks)]\n",
    "arrays = [da.from_delayed(obj, shape=(n_samples_per_block, n_features), dtype='float64')\n",
    "          for obj in delayeds]\n",
    "X = da.concatenate(arrays)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16.0"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the size of the array\n",
    "X.nbytes / 1e9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only run this on the cluster.\n",
    "X = X.persist()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithms implemented in Dask-ML are scalable. They handle larger-than-memory datasets just fine.\n",
    "\n",
    "They follow the scikit-learn API, so if you're familiar with scikit-learn, you'll feel at home with Dask-ML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_ml.cluster import KMeans\n",
    "clf = KMeans(init_max_iter=3, oversampling_factor=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14.4 s, sys: 351 ms, total: 14.8 s\n",
      "Wall time: 44 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='full', copy_x=True, init='k-means||', init_max_iter=3,\n",
       "       max_iter=300, n_clusters=8, n_jobs=1, oversampling_factor=10,\n",
       "       precompute_distances='auto', random_state=None, tol=0.0001)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time clf.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dask.array<astype, shape=(100000000,), dtype=int32, chunksize=(200000,)>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 0, 4, 7, 7, 7, 2, 0, 3, 6], dtype=int32)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.labels_[:10].compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shut cluster down\n",
    "To shut the cluster down, cancel the job that runs the cluster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.cancel()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dask-tutorial]",
   "language": "python",
   "name": "conda-env-dask-tutorial-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
